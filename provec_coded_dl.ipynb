{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df2022-d6d2-4b8a-8bc3-281a98ce7ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc8871-4cff-4fca-9a3f-30f64d83c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the genotype_fitness_data.tsv file\n",
    "file_path = 'C:/Users/Thomascrx/Desktop/ml_code/sequence_points_file.csv'  # Replace with your file path\n",
    "genotype_fitness_data = pd.read_csv(file_path)\n",
    "print(genotype_fitness_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579b928-c109-458c-a0c8-e518c06456c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coded_by_provec\n",
    "def split_overlapping(s, window_size):\n",
    "    # 使用列表推导式生成重叠的子字符串\n",
    "    return [s[i:i + window_size] for i in range(len(s) - window_size + 1)]\n",
    "\n",
    "def cut_to_provec(sequence,num):\n",
    "   return [split_overlapping(i, window_size=num) for i in sequence]\n",
    "    \n",
    "sequence=genotype_fitness_data[\"sequence\"]\n",
    "sequence=cut_to_provec(sequence,3)                              \n",
    "provec=pd.read_csv(\"C:/Users/Thomascrx/Desktop/protVec_100d_3grams.csv\",sep=\"\\t\")\n",
    "# 指定要合并的列\n",
    "columns=list(provec.columns)\n",
    "columns.pop(0)\n",
    "columns_to_merge = columns\n",
    "# 对每一行操作，将指定列的值合并成一个列表，并存储到新列中\n",
    "provec['Merged'] = provec[columns_to_merge].apply(lambda row: row.tolist(), axis=1)\n",
    "# 删除已合并的列\n",
    "provec.drop(columns=columns_to_merge, inplace=True)\n",
    "provec_dict = provec.set_index('words')['Merged'].to_dict()\n",
    "\n",
    "def provec_encode_aa(sequence):\n",
    "    return np.array([provec_dict[aa] for aa in sequence])\n",
    "\n",
    "\n",
    "X = np.array([provec_encode_aa(aa) for aa in sequence])\n",
    "y = genotype_fitness_data['delta_log10Ka'].values\n",
    "print(f\"X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48e198-660a-447e-b2ae-b8bc01e50fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
    "# 标准化 y（使用训练集的均值和标准差）\n",
    "y_mean = y_train.mean()\n",
    "y_std = y_train.std()\n",
    "y_train = (y_train - y_mean) / y_std\n",
    "y_test = (y_test - y_mean) / y_std\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "print(f\"y_train_tensor shape after view: {y_train_tensor.shape}\")\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af10aef-3810-405a-973f-2b4b80c9d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "class Config:\n",
    "    seq_len = 201          # 序列长度\n",
    "    vocab_size = 20       # 氨基酸种类数（独热编码维度）\n",
    "    d_model = 256         # 嵌入维度\n",
    "    nhead = 8             # 注意力头数\n",
    "    num_layers = 4        # 编码器层数\n",
    "    dim_feedforward = 1024 # 前馈网络维度\n",
    "    dropout = 0.1         # Dropout 比率\n",
    "    batch_size = 64       # Batch Size（根据 GPU 显存调整）\n",
    "    lr = 3e-4             # 学习率\n",
    "    weight_decay = 0.01   # 权重衰减\n",
    "    epochs = 100          # 训练轮数\n",
    "    grad_clip = 5.0       # 梯度裁剪阈值\n",
    "    log_dir = \"runs/exp1\" # TensorBoard 日志目录\n",
    "\n",
    "# 模型定义\n",
    "class ProteinTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(config.vocab_size, config.d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.nhead,\n",
    "            dim_feedforward=config.dim_feedforward,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
    "        self.fc = nn.Linear(config.seq_len * config.d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)          # (batch, 201, 20) → (batch, 201, 256)\n",
    "        x = x.permute(1, 0, 2)         # (201, batch, 256)\n",
    "        x = self.encoder(x)            # (201, batch, 256)\n",
    "        x = x.permute(1, 0, 2)         # (batch, 201, 256)\n",
    "        x = x.reshape(x.size(0), -1)   # (batch, 201 * 256=51456)\n",
    "        return self.fc(x)              # (batch, 1)\n",
    "\n",
    "# 初始化模型和优化器\n",
    "config = Config()\n",
    "model = ProteinTransformer(config)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# 多 GPU 支持\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "criterion = nn.MSELoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 初始化 TensorBoard 写入器\n",
    "writer = SummaryWriter(log_dir=config.log_dir)\n",
    "\n",
    "# 示例数据加载（假设 X, y 已预处理为 Tensor）\n",
    "dataset = TensorDataset(torch.randn(136204, 201, 20), torch.randn(136204, 1))\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (batch_X, batch_y) in enumerate(dataloader):\n",
    "        batch_X, batch_y = batch_X.to(\"cuda\"), batch_y.to(\"cuda\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 混合精度前向传播\n",
    "        with autocast():\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # 反向传播与梯度裁剪\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # 记录 batch 级损失\n",
    "        current_step = epoch * len(dataloader) + batch_idx\n",
    "        if batch_idx % 10 == 0:\n",
    "            writer.add_scalar(\"Loss/train (batch)\", loss.item(), current_step)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 记录 epoch 级平均损失\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    writer.add_scalar(\"Loss/train (epoch)\", avg_loss, epoch)\n",
    "    print(f\"Epoch {epoch}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 关闭 TensorBoard 写入器\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
